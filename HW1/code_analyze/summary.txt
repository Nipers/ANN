########################
# Additional Files
########################
# plot.py
# data
# readme.txt
# __pycache__

########################
# Filled Code
########################
# ..\codes\layers.py:1
        self._saved_for_backward(input)
        return np.maximum(0, input)

# ..\codes\layers.py:2
        grad_output[self._saved_tensor < 0] = 0
        return grad_output

# ..\codes\layers.py:3
        self._saved_tensor = input
        return 1 / (1 + np.exp(-input))

# ..\codes\layers.py:4
        sig = self.forward(self._saved_tensor)
        return grad_output * sig * (1 - sig)

# ..\codes\layers.py:5
        self._saved_tensor = input
        u = np.power(2/np.pi, 0.5) * (input + 0.044715*(np.power(input, 3)))
        return 0.5 * input *(1 + np.tanh(u))

# ..\codes\layers.py:6
        x = np.power(self._saved_tensor, 3)
        a = 0.0356774 * x + 0.797885 * self._saved_tensor
        b = 0.0535161 * x + 0.398942 * self._saved_tensor
        sec = 2 / (np.exp(a) + np.exp(-a))
        return grad_output * (0.5 * np.tanh(a) + b * np.power(sec, 2) + 0.5)

# ..\codes\layers.py:7
        self._saved_tensor = input
        return input @ self.W + self.b

# ..\codes\layers.py:8
        self.grad_b = -grad_output
        self.grad_W = -self._saved_tensor.T @ grad_output / grad_output.shape[0]
        return grad_output @ self.W.T

# ..\codes\loss.py:1
        normed_diff = np.linalg.norm(target - input, axis = 1)
        return (np.power(normed_diff, 2)).mean(axis=0).sum() / 2.0

# ..\codes\loss.py:2
        return target - input
        # return (target - input) / input.shape[0]

# ..\codes\loss.py:3
        exp = np.exp(input - np.max(input))
        self.soft = exp / np.expand_dims(np.sum(exp, 1),-1)
        return -np.mean(np.log(self.soft) * target)

# ..\codes\loss.py:4
        return target - self.soft
        # return (target - input) / input.shape[0]

# ..\codes\loss.py:5
        hk = np.maximum(0, self.margin - input[target == 1].reshape(-1, 1) + input)
        En = np.sum((target == 0) * hk, axis=1)
        return np.mean(En)

# ..\codes\loss.py:6
        grad = np.zeros(input.shape, input.dtype)
        # grad = np.zeros_like()
        grad[(target == 0) & ((self.margin - input[target == 1].reshape(-1, 1) + input) > 0)] = -1
        grad[target == 1] = -np.sum(grad, axis = 1)
        return grad
        # return grad / input.shape[0]


########################
# References
########################

########################
# Other Modifications
########################
# _codes\network.py -> ..\codes\network.py
# 27 +     def __str__(self) -> str:
# 28 +         res = ""
# 29 +         for layer in self.layer_list:
# 30 +             res += str(layer)
# 31 +             res += "_"
# 32 +         return res
# _codes\layers.py -> ..\codes\layers.py
# 40 +     def __str__(self) -> str:
# 41 +         return "R"
# 58 +     def __str__(self) -> str:
# 59 +         return "S"
# 60 +
# 81 +
# 82 +     def __str__(self) -> str:
# 83 +         return "G"
# 103 -
# 123 +
# 124 +     def __str__(self) -> str:
# 125 +         return "L"
# _codes\loss.py -> ..\codes\loss.py
# 3 + from numpy.lib.function_base import diff
# 21 +     def __str__(self) -> str:
# 22 +         return "E"
# 28 +         self.loss = np.zeros(1, dtype="f")
# 42 +     def __str__(self) -> str:
# 43 +         return "S"
# 49 +         self.margin = margin
# 68 +     def __str__(self) -> str:
# 69 +         return "H"
# _codes\solve_net.py -> ..\codes\solve_net.py
# 20 +     l1 = []
# 21 +     l2 = []
# 45 +             l1.append(np.mean(loss_list))
# 46 +             l2.append(np.mean(acc_list))
# 45 -             LOG_INFO(msg)
# 49 +             # LOG_INFO(msg)
# 49 ?            ++
# 50 +     return np.mean(l1), np.mean(l2)
# 61 -     LOG_INFO(msg)
# 66 +     # LOG_INFO(msg)
# 66 ?    ++
# 67 +     return np.mean(loss_list), np.mean(acc_list)
# _codes\run_mlp.py -> ..\codes\run_mlp.py
# 13 + # models = []
# 14 + # losses = []
# 15 + # for i in range(0,6):
# 16 + #     if i < 3:#one layer
# 13 - model = Network()
# 17 + #         model = Network()
# 17 ? ++++++++++
# 14 - model.add(Linear('fc1', 784, 10, 0.01))
# 14 ?                               ^
# 18 + #         model.add(Linear('fc1', 784, 128, 0.01))
# 18 ? ++++++++++                              ^^
# 19 + #         if i == 0:
# 20 + #             model.add(Relu("r1"))
# 21 + #         if i == 1:
# 22 + #             model.add(Gelu("r1"))
# 23 + #         if i == 2:
# 24 + #             model.add(Sigmoid("r1"))
# 25 + #         model.add(Linear('fc1', 128, 10, 0.01))
# 26 + #         print(str(model))
# 27 + #         models.append(model)
# 28 + #     else:#two layer
# 29 + #         model = Network()
# 30 + #         model.add(Linear('fc1', 784, 256, 0.01))
# 31 + #         model.add(Relu("r1"))
# 32 + #         model.add(Linear('fc1', 256, 64, 0.01))
# 33 + #         if i == 3:
# 34 + #             model.add(Relu("r1"))
# 35 + #         if i == 4:
# 36 + #             model.add(Gelu("r1"))
# 37 + #         if i == 5:
# 38 + #             model.add(Sigmoid("r1"))
# 39 + #         model.add(Linear('fc1', 64, 10, 0.01))
# 40 + #         print(str(model))
# 41 + #         models.append(model)
# 43 + # losses.append(SoftmaxCrossEntropyLoss(name='loss'))
# 16 - loss = EuclideanLoss(name='loss')
# 16 ?     ^^^
# 44 + # losses.append(EuclideanLoss(name='loss'))
# 44 ? ++    ^^^^^^^^^^                          +
# 45 + # losses.append(HingeLoss(name='loss'))
# 46 +
# 23 -
# 53 + model = Network()
# 54 + model.add(Linear('fc1', 784, 128, 0.01))
# 55 + model.add(Gelu("r1"))
# 56 + model.add(Linear('fc1', 128, 10, 0.01))
# 57 + print(str(model))
# 58 + loss = SoftmaxCrossEntropyLoss(name='loss')
# 25 -     'learning_rate': 0.0,
# 60 +     'learning_rate': 0.003,
# 60 ?                         ++
# 28 -     'batch_size': 100,
# 28 ?                   ^^
# 63 +     'batch_size': 50,
# 63 ?                   ^
# 29 -     'max_epoch': 100,
# 29 ?                  ^
# 64 +     'max_epoch': 200,
# 64 ?                  ^
# 34 -
# 69 + for i in range(0, 6):
# 70 +     for j in range(0, 3):
# 71 +         index = i * 3 + j
# 72 +         acc_train = "accuracy/train/"
# 73 +         acc_test = "accuracy/test/"
# 74 +         loss_train = "loss/train/"
# 75 +         loss_test = "loss/test/"
# 76 +         print(str(model) + str(loss))
# 35 - for epoch in range(config['max_epoch']):
# 77 +         for epoch in range(config['max_epoch']):
# 77 ? ++++++++
# 36 -     LOG_INFO('Training @ %d epoch...' % (epoch))
# 78 +             LOG_INFO('Training @ %d epoch...' % (epoch))
# 78 ? ++++++++
# 37 -     train_net(model, loss, config, train_data, train_label, config['batch_size'], config['disp_freq'])
# 79 +             l1, l2 = train_net(model, loss, config, train_data, train_label, config['batch_size'], config['disp_freq'])
# 79 ?    +++++++++++++++++
# 38 -
# 80 +             # fileName = loss_train + str(0.07) + ".txt"
# 81 +             # file = open(fileName, "a")
# 82 +             # file.write(str(l1) + "\n")
# 83 +             # fileName = acc_train + str(0.07) + ".txt"
# 84 +             # file = open(fileName, "a")
# 85 +             # file.write(str(l2) + "\n")
# 86 +             # file.close()
# 87 +             print(l2)
# 39 -     if epoch % config['test_epoch'] == 0:
# 88 +             if epoch % config['test_epoch'] == 0:
# 88 ? ++++++++
# 40 -         LOG_INFO('Testing @ %d epoch...' % (epoch))
# 89 +                 LOG_INFO('Testing @ %d epoch...' % (epoch))
# 89 ? ++++++++
# 41 -         test_net(model, loss, test_data, test_label, config['batch_size'])
# 90 +                 l, t = test_net(model, loss, test_data, test_label, config['batch_size'])
# 90 ?        +++++++++++++++
# 91 +                 # fileName = loss_test + str(0.075) + ".txt"
# 92 +                 # file = open(fileName, "a")
# 93 +                 # file.write(str(l) + "\n")
# 94 +                 # file.close()
# 95 +                 # fileName = acc_test + str(0.07) + ".txt"
# 96 +                 # file = open(fileName, "a")
# 97 +                 # file.write(str(t) + "\n")
# 98 +                 # file.close()

