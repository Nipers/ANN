########################
# Additional Files
########################
# __pycache__
# data
# accuracy
# loss

########################
# Filled Code
########################
# ..\codes\loss.py:1
        normed_diff = np.linalg.norm(target - input, axis = 1)
        return (np.power(normed_diff, 2)).mean(axis=0).sum() / 2.0

    def backward(self, input, target):
        return target - input
        # return (target - input) / input.shape[0]

# ..\codes\loss.py:2
        grad = np.zeros_like(input)
        grad[(target == 0) & (self.margin - input[target == 1].reshape(-1, 1) + input > 0)] = -1
        grad[(target == 1)] = -np.sum(grad, axis=1)
        return grad
        # return grad / input.shape[0]

# ..\codes\layers.py:1
        self._saved_for_backward(input)
        return np.maximum(0, input)

# ..\codes\layers.py:2
        grad_output[self._saved_tensor < 0] = 0
        return grad_output

# ..\codes\layers.py:3
        self._saved_tensor = input
        return 1 / (1 + np.exp(-input))

# ..\codes\layers.py:4
        sig = self.forward(self._saved_tensor)
        return grad_output * sig * (1 - sig)

# ..\codes\layers.py:5
        self._saved_tensor = input
        u = np.power(2/np.pi, 0.5) * (input + 0.044715*(np.power(input, 3)))
        return 0.5 * input *(1 + np.tanh(u))

# ..\codes\layers.py:6
        x = np.power(self._saved_tensor, 3)
        a = 0.0356774 * x + 0.797885 * self._saved_tensor
        b = 0.0535161 * x + 0.398942 * self._saved_tensor
        sec = 2 / (np.exp(a) + np.exp(-a))
        return grad_output * (0.5 * np.tanh(a) + b * np.power(sec, 2) + 0.5)

    def __str__(self) -> str:
        return "G"
        self._saved_tensor = input
        return input @ self.W + self.b

# ..\codes\layers.py:7
        self.grad_b = -grad_output
        self.grad_W = -self._saved_tensor.T @ grad_output / grad_output.shape[0]
        return grad_output @ self.W.T


########################
# References
########################

########################
# Other Modifications
########################
# _codes\run_mlp.py -> ..\codes\run_mlp.py
# 13 + models = []
# 14 + losses = []
# 15 + for i in range(0,6):
# 16 +     if i < 3:#one layer
# 13 - model = Network()
# 17 +         model = Network()
# 17 ? ++++++++
# 14 - model.add(Linear('fc1', 784, 10, 0.01))
# 14 ?                               ^
# 18 +         model.add(Linear('fc1', 784, 128, 0.01))
# 18 ? ++++++++                              ^^
# 19 +         if i == 0:
# 20 +             model.add(Relu("r1"))
# 21 +         if i == 1:
# 22 +             model.add(Gelu("r1"))
# 23 +         if i == 2:
# 24 +             model.add(Sigmoid("r1"))
# 25 +         model.add(Linear('fc1', 128, 10, 0.01))
# 26 +         print(str(model))
# 27 +         models.append(model)
# 28 +     else:#two layer
# 29 +         model = Network()
# 30 +         model.add(Linear('fc1', 784, 256, 0.01))
# 31 +         model.add(Relu("r1"))
# 32 +         model.add(Linear('fc1', 256, 64, 0.01))
# 33 +         if i == 3:
# 34 +             model.add(Relu("r1"))
# 35 +         if i == 4:
# 36 +             model.add(Gelu("r1"))
# 37 +         if i == 5:
# 38 +             model.add(Sigmoid("r1"))
# 39 +         model.add(Linear('fc1', 64, 10, 0.01))
# 40 +         print(str(model))
# 41 +         models.append(model)
# 43 + losses.append(SoftmaxCrossEntropyLoss(name='loss'))
# 16 - loss = EuclideanLoss(name='loss')
# 16 ?     ^^^
# 44 + losses.append(EuclideanLoss(name='loss'))
# 44 ?     ^^^^^^^^^^                          +
# 45 + losses.append(HingeLoss(name='loss'))
# 46 +
# 25 -     'learning_rate': 0.0,
# 55 +     'learning_rate': 0.003,
# 55 ?                         ++
# 29 -     'max_epoch': 100,
# 29 ?                  ^
# 59 +     'max_epoch': 200,
# 59 ?                  ^
# 34 -
# 64 + for i in range(0, 6):
# 65 +     for j in range(0, 3):
# 66 +         index = i * 3 + j
# 67 +         model = models[i]
# 68 +         loss = losses[j]
# 69 +         acc_train = "accuracy/train/"
# 70 +         acc_test = "accuracy/test/"
# 71 +         loss_train = "loss/train/"
# 72 +         loss_test = "loss/test/"
# 73 +         print(str(model) + str(loss))
# 35 - for epoch in range(config['max_epoch']):
# 74 +         for epoch in range(config['max_epoch']):
# 74 ? ++++++++
# 36 -     LOG_INFO('Training @ %d epoch...' % (epoch))
# 75 +             LOG_INFO('Training @ %d epoch...' % (epoch))
# 75 ? ++++++++
# 37 -     train_net(model, loss, config, train_data, train_label, config['batch_size'], config['disp_freq'])
# 76 +             l1, l2 = train_net(model, loss, config, train_data, train_label, config['batch_size'], config['disp_freq'])
# 76 ?    +++++++++++++++++
# 38 -
# 77 +             fileName = loss_train + str(model) + str(loss)+".txt"
# 78 +             file = open(fileName, "a")
# 79 +             file.write(str(l1) + "\n")
# 80 +             fileName = acc_train + str(model) + str(loss)+".txt"
# 81 +             file = open(fileName, "a")
# 82 +             file.write(str(l2) + "\n")
# 39 -     if epoch % config['test_epoch'] == 0:
# 83 +             if epoch % config['test_epoch'] == 0:
# 83 ? ++++++++
# 40 -         LOG_INFO('Testing @ %d epoch...' % (epoch))
# 84 +                 LOG_INFO('Testing @ %d epoch...' % (epoch))
# 84 ? ++++++++
# 41 -         test_net(model, loss, test_data, test_label, config['batch_size'])
# 85 +                 l, t = test_net(model, loss, test_data, test_label, config['batch_size'])
# 85 ?        +++++++++++++++
# 86 +                 fileName = loss_test + str(model) + str(loss)+ ".txt"
# 87 +                 file = open(fileName, "a")
# 88 +                 file.write(str(l) + "\n")
# 89 +                 file.close()
# 90 +                 fileName = acc_test + str(model) + str(loss)+ ".txt"
# 91 +                 file = open(fileName, "a")
# 92 +                 file.write(str(t) + "\n")
# 93 +                 file.close()
# _codes\loss.py -> ..\codes\loss.py
# 3 + from numpy.lib.function_base import diff
# 20 +     def __str__(self) -> str:
# 21 +         return "E"
# 22 +
# 23 +
# 24 + class SoftmaxCrossEntropyLoss(object):
# 25 +     def __init__(self, name):
# 26 +         self.name = name
# 27 +         self.loss = np.zeros(1, dtype="f")
# 28 +
# 29 +     def forward(self, input, target):
# 35 +
# 36 +     def backward(self, input, target):
# 41 +     def __str__(self) -> str:
# 42 +         return "S"
# 43 +
# 44 +
# 45 + class HingeLoss(object):
# 46 +     def __init__(self, name, margin=5):
# 47 +         self.name = name
# 48 +         self.margin = margin
# 49 +
# 50 +     def forward(self, input, target):
# 65 +     def __str__(self) -> str:
# 66 +         return "H"
# 21 -
# 22 - class SoftmaxCrossEntropyLoss(object):
# 23 -     def __init__(self, name):
# 24 -         self.name = name
# 25 -
# 26 -     def forward(self, input, target):
# 31 -
# 32 -     def backward(self, input, target):
# 37 -
# 38 -
# 39 - class HingeLoss(object):
# 40 -     def __init__(self, name, margin=5):
# 41 -         self.name = name
# 42 -
# 43 -     def forward(self, input, target):
# 48 -
# 49 -     def backward(self, input, target):
# 54 -
# _codes\layers.py -> ..\codes\layers.py
# 40 +     def __str__(self) -> str:
# 41 +         return "R"
# 58 +     def __str__(self) -> str:
# 59 +         return "S"
# 60 +
# 73 +         x = np.power(self._saved_tensor, 3)
# 74 +         a = 0.0356774 * x + 0.797885 * self._saved_tensor
# 75 +         b = 0.0535161 * x + 0.398942 * self._saved_tensor
# 76 +         sec = 2 / (np.exp(a) + np.exp(-a))
# 77 +         return grad_output * (0.5 * np.tanh(a) + b * np.power(sec, 2) + 0.5)
# 78 +
# 79 +     def __str__(self) -> str:
# 80 +         return "G"
# 103 -
# 120 +
# 121 +     def __str__(self) -> str:
# 122 +         return "L"
# _codes\solve_net.py -> ..\codes\solve_net.py
# 20 +     l1 = []
# 21 +     l2 = []
# 45 +             l1.append(np.mean(loss_list))
# 46 +             l2.append(np.mean(acc_list))
# 45 -             LOG_INFO(msg)
# 49 +             # LOG_INFO(msg)
# 49 ?            ++
# 50 +     return np.mean(l1), np.mean(l2)
# 61 -     LOG_INFO(msg)
# 66 +     # LOG_INFO(msg)
# 66 ?    ++
# 67 +     return np.mean(loss_list), np.mean(acc_list)
# _codes\network.py -> ..\codes\network.py
# 27 +     def __str__(self) -> str:
# 28 +         res = ""
# 29 +         for layer in self.layer_list:
# 30 +             res += str(layer)
# 31 +             res += "_"
# 32 +         return res

